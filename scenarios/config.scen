# A sample scenario file for the MrBen
[GENERAL]
# Set name of the configuration
config.name=“sample-config”
# Set default user for access server
config.user=“root”
# Set default private key for access server
config.key=“privatekey.cer”

[NODES]
# List of nodes joining the test can be specified directly in this section
# hostname:user:privatekey
# hostname:: means use default setting for the host
vm1:user:user.key  # user and key are not the default one
vm2:user:          # user are not the default one, key is default
vm3:ubuntu:ubuntu.key
vm4::ubuntu.key    # user is default, key is not 

[NETWORK]
# Scenario for network measurement
# syntax: sender>receiver:protocol:duration/size:num_connections
# sender: hostname of sender
# receiver: hostname of receiver
# protocol: tcp or udp
# duration: time for test connection in second: s
# size: size to transfer for each connection in byte: K,M,G
# parallel: send in parallel; series: send in series from top to bottom
# when there are multiple blocks, run each block then move to next block (like series)
output="networktest.out"
[parallel]
vm1>vm2:tcp:60s:2 
vm2>vm3:udp:20M:1
[series]
vm1>vm2:tcp:60s:2 
vm2>vm3:udp:20M:2
vm3>vm4:tcp:60s:2
[parallel]
vm1>vm2:tcp:60s:2 
vm2>vm3:udp:20M:2
vm3>vm4:tcp:60s:2

# or using network communication pattern in Hadoop to run measurement
[HADOOP-PATTERN-NETWORK]
# List of patterns:
#     broadcast(source): all other node is receiver, source is sender
#     shuffle(mapper(vm1,vm2,vm3),reducer(vm1,vm2,vm3)): all mappers send to one reducer = m.r flows
#     aggregate(contributor(vm1,vm2,vm3),aggregator(vm1)): like shuffle but having only one reducer
#     parallelrw(pair(vm1,vm1),pair(vm2,vm2),pair(vm3,vm3)): send in pair
#     one2one(vm1,vm2):
#     all2all(): like shuffle, but all nodes are mappers and reducers
output="hadooptest.out"
broadcast(vm1):tcp:60s:1
shuffle(mapper(vm1,vm2,vm3),reducer(vm1,vm2,vm3)):tcp:30s:1

# Scenario for disk measurement
[DISK]
# There will be some files to choose.
#     randomseek.fio: one thread read/write by 50%/50% randomly access block
#     hadoop.fio: one thread sequence read, 128 MBs
#     hadoopstress.fio: 4 threads sequence read/write 3 read/1 write, 128MBs
disk.config=“hadoopintensive.fio”
output="disktest.out"
mode="concurrent" #or "continuous"
# By default, test all the nodes, if want to exclude nodes, put them in ignoring block
[ignore]
vm2

[[PLUGIN_SETTING]]
# List of node can be generated by using OpenStack or EC2 API
[OPENSTACK] #optional
# OpenStack Setting
[create]
vm1:node1          # create vm1 on node1
vm2:node2          # create vm2 on node2
vm3:               # create vm2 on a random node
[other_info]
cloudimage=""      # path to cloud image
osusername=""      # openstack username
oskeyfile=""       # openstack access key
accesslink=""      # accessing link to openstack platform
openrcfile=""      # path to open rc file

[AWS] #optional
# AWS setting

[HADOOP] #optional
# List of node can also be learnt from Hadoop cluster (YARN or HDFS members)